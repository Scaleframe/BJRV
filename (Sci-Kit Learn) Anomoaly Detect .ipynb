{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Now let us look at anomaly detection. The following is adapted from the Kaggle notebook:\n",
    "## https://www.kaggle.com/victorambonati/unsupervised-anomaly-detection/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rentals_df = pd.read_csv('rentals.csv', encoding='latin-1')\n",
    "rentals_df.head()\n",
    "\n",
    "stations_df = pd.read_csv('stations.csv', encoding='latin-1')\n",
    "stations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "#from pyemma import msm # not available on Kaggle Kernel\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return Series of distance between each point and his distance with the closest centroid\n",
    "def getDistanceByPoint(data, model):\n",
    "    distance = pd.Series()\n",
    "    for i in range(0,len(data)):\n",
    "        Xa = np.array(data.loc[i])\n",
    "        Xb = model.cluster_centers_[model.labels_[i]-1]\n",
    "        distance.set_value(i, np.linalg.norm(Xa-Xb))\n",
    "    return distance\n",
    "\n",
    "# train markov model to get transition matrix\n",
    "def getTransitionMatrix (df):\n",
    "\tdf = np.array(df)\n",
    "\tmodel = msm.estimate_markov_model(df, 1)\n",
    "\treturn model.transition_matrix\n",
    "\n",
    "def markovAnomaly(df, windows_size, threshold):\n",
    "    transition_matrix = getTransitionMatrix(df)\n",
    "    real_threshold = threshold**windows_size\n",
    "    df_anomaly = []\n",
    "    for j in range(0, len(df)):\n",
    "        if (j < windows_size):\n",
    "            df_anomaly.append(0)\n",
    "        else:\n",
    "            sequence = df[j-windows_size:j]\n",
    "            sequence = sequence.reset_index(drop=True)\n",
    "            df_anomaly.append(anomalyElement(sequence, real_threshold, transition_matrix))\n",
    "    return df_anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the hours and if it's night or day (7:00-22:00)\n",
    "rentals_df['hours'] = rentals_df['Starttime_dt'].dt.hour\n",
    "rentals_df['daylight'] = ((rentals_df['hours'] >= 7) & (rentals_df['hours'] <= 22)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the day of the week (Monday=0, Sunday=6) and if it's a week end day or week day.\n",
    "rentals_df['DayOfTheWeek'] = rentals_df['Starttime_dt'].dt.dayofweek\n",
    "rentals_df['WeekDay'] = (rentals_df['DayOfTheWeek'] < 5).astype(int)\n",
    "# An estimation of anomly population of the dataset (necessary for several algorithm)\n",
    "outliers_fraction = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creation of 4 distinct categories that seem useful (week end/day week & night/day)\n",
    "rentals_df['Starttime_cat'] = rentals_df['WeekDay']*2 + rentals_df['daylight']\n",
    "\n",
    "a = rentals_df.loc[rentals_df['Starttime_cat'] == 0, 'Tripduration_mins']\n",
    "b = rentals_df.loc[rentals_df['Starttime_cat'] == 1, 'Tripduration_mins']\n",
    "c = rentals_df.loc[rentals_df['Starttime_cat'] == 2, 'Tripduration_mins']\n",
    "d = rentals_df.loc[rentals_df['Starttime_cat'] == 3, 'Tripduration_mins']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "a_heights, a_bins = np.histogram(a)\n",
    "b_heights, b_bins = np.histogram(b, bins=a_bins)\n",
    "c_heights, c_bins = np.histogram(c, bins=a_bins)\n",
    "d_heights, d_bins = np.histogram(d, bins=a_bins)\n",
    "\n",
    "width = (a_bins[1] - a_bins[0])/6\n",
    "\n",
    "ax.bar(a_bins[:-1], a_heights*100/a.count(), width=width, facecolor='blue', label='WeekEnd Night')\n",
    "ax.bar(b_bins[:-1]+width, (b_heights*100/b.count()), width=width, facecolor='green', label ='WeekEndDayLight')\n",
    "ax.bar(c_bins[:-1]+width*2, (c_heights*100/c.count()), width=width, facecolor='red', label ='WeekDay Night')\n",
    "ax.bar(d_bins[:-1]+width*3, (d_heights*100/d.count()), width=width, facecolor='black', label ='WeekDay DayLight')\n",
    "\n",
    "ax.set_xlabel('Trip Duration in minutes')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Now we have to be careful in choice of outlier detector. What is the nature of our data? It is partly \n",
    "### unordered (Bike IDs, Trip IDs, Stations) and partly ordered (timestamps of start and stop of trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Let us start with simple clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rentals_df['Starttime_num']=mdates.date2num(rentals_df['Starttime_dt'].astype(datetime))\n",
    "rentals_df['Stoptime_num']=mdates.date2num(rentals_df['Stoptime_dt'].astype(datetime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pull out data for PCA analysis\n",
    "data = rentals_df[['Tripduration', 'Starttime_num', 'Stoptime_num', 'From station id', 'To station id', \\\n",
    "                   'hours', 'daylight', 'DayOfTheWeek', 'WeekDay']]\n",
    "min_max_scaler = preprocessing.StandardScaler()\n",
    "np_scaled = min_max_scaler.fit_transform(data)\n",
    "data = pd.DataFrame(np_scaled)\n",
    "# reduce to 2 importants features\n",
    "pca = PCA(n_components=2)\n",
    "data = pca.fit_transform(data)\n",
    "# standardize these 2 new features\n",
    "min_max_scaler = preprocessing.StandardScaler()\n",
    "np_scaled = min_max_scaler.fit_transform(data)\n",
    "data = pd.DataFrame(np_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate with different number of centroids to see the loss plot (elbow method)\n",
    "n_cluster = range(1, 20)\n",
    "kmeans = [KMeans(n_clusters=i).fit(data) for i in n_cluster]\n",
    "scores = [kmeans[i].score(data) for i in range(len(kmeans))]\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(n_cluster, scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# choose 15 centroids arbitrarily and add these data to the central dataframe\n",
    "rentals_df['cluster'] = kmeans[14].predict(data)\n",
    "rentals_df['principal_feature1'] = data[0]\n",
    "rentals_df['principal_feature2'] = data[1]\n",
    "rentals_df['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot the different clusters with the 2 main features\n",
    "fig, ax = plt.subplots()\n",
    "colors = {0:'red', 1:'blue', 2:'green', 3:'pink', 4:'black', 5:'orange', 6:'cyan', 7:'yellow', 8:'brown', \\\n",
    "          9:'purple', 10:'white', 11: 'grey', 12:'lightblue', 13:'lightgreen', 14: 'darkgrey'}\n",
    "ax.scatter(rentals_df['principal_feature1'], rentals_df['principal_feature2'], \n",
    "           c=rentals_df[\"cluster\"].apply(lambda x: colors[x]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets zoom in separately to see clearer\n",
    "fig, ax = plt.subplots(1,2,figsize=(15,8))\n",
    "colors = {0:'red', 1:'blue', 2:'green', 3:'pink', 4:'black', 5:'orange', 6:'cyan', 7:'yellow', 8:'brown', \\\n",
    "          9:'purple', 10:'white', 11: 'grey', 12:'lightblue', 13:'lightgreen', 14: 'darkgrey'}\n",
    "ax[0].scatter(rentals_df['principal_feature1'], rentals_df['principal_feature2'], \n",
    "           c=rentals_df[\"cluster\"].apply(lambda x: colors[x]))\n",
    "ax[0].set_xlim(-2,4.1)\n",
    "ax[0].set_ylim(-2,5)\n",
    "ax[0].set_title('PCA Analysis (Lower quadrant)')\n",
    "ax[1].scatter(rentals_df['principal_feature1'], rentals_df['principal_feature2'], \n",
    "           c=rentals_df[\"cluster\"].apply(lambda x: colors[x]))\n",
    "ax[1].set_xlim(40,55)\n",
    "ax[1].set_ylim(50,65)\n",
    "ax[1].set_title('PCA Analysis (Upper quadrant)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the distance between each point and its nearest centroid. \n",
    "# The biggest distances are considered as anomalies\n",
    "distance = getDistanceByPoint(data, kmeans[14])\n",
    "number_of_outliers = int(outliers_fraction*len(distance))\n",
    "threshold = distance.nlargest(number_of_outliers).min()\n",
    "rentals_df['anomaly21'] = (distance >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualisation of anomaly with cluster view\n",
    "fig, ax = plt.subplots()\n",
    "colors = {0:'blue', 1:'red'}\n",
    "ax.scatter(rentals_df['principal_feature1'], rentals_df['principal_feature2'], \\\n",
    "           c=rentals_df[\"anomaly21\"].apply(lambda x: colors[x]))\n",
    "ax.set_title('Outlier PCA Analysis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anomalous_rides=rentals_df[rentals_df['anomaly21']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# repeat visualization with zooms, as before\n",
    "fig, ax = plt.subplots(1,2,figsize=(15,8))\n",
    "colors = {0:'blue', 1:'red'}\n",
    "ax[0].scatter(rentals_df['principal_feature1'], rentals_df['principal_feature2'], \n",
    "           c=rentals_df[\"anomaly21\"].apply(lambda x: colors[x]))\n",
    "ax[0].set_xlim(-2,4.1)\n",
    "ax[0].set_ylim(-2,5)\n",
    "ax[0].set_title('Outlier PCA Analysis (Lower quadrant)')\n",
    "ax[1].scatter(rentals_df['principal_feature1'], rentals_df['principal_feature2'], \n",
    "           c=rentals_df[\"anomaly21\"].apply(lambda x: colors[x]))\n",
    "ax[1].set_xlim(40,55)\n",
    "ax[1].set_ylim(50,65)\n",
    "ax[1].annotate(anomalous_rides['Trip id'],xy=(anomalous_rides['principal_feature1'],anomalous_rides['principal_feature2']))\n",
    "ax[1].set_title('Outlier PCA Analysis (Upper quadrant)')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anomalous_rides.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now lets try an isolation-forest algorithm, from Scikit-Learn\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pull out data for Isolation Forests analysis\n",
    "data = rentals_df[['Tripduration', 'Starttime_num', 'Stoptime_num', 'From station id', 'To station id', \\\n",
    "                   'hours', 'daylight', 'DayOfTheWeek', 'WeekDay']]\n",
    "min_max_scaler = preprocessing.StandardScaler()\n",
    "np_scaled = min_max_scaler.fit_transform(data)\n",
    "data = pd.DataFrame(np_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit the model\n",
    "clf = IsolationForest(contamination = outliers_fraction)\n",
    "clf.fit(data)\n",
    "rentals_df['anomaly_if'] = pd.Series(clf.predict(data))\n",
    "rentals_df['anomaly_if'] = rentals_df['anomaly_if'].map( {1: 0, -1: 1} )\n",
    "print(rentals_df['anomaly_if'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bikeid_min=rentals_df['Bikeid'].min()\n",
    "bikeid_max=rentals_df['Bikeid'].max()\n",
    "tripid_min=rentals_df['Trip id'].min()\n",
    "tripid_max=rentals_df['Trip id'].max()\n",
    "fromstationid_min=rentals_df['From station id'].min()\n",
    "fromstationid_max=rentals_df['From station id'].max()\n",
    "tostationid_min=rentals_df['To station id'].min()\n",
    "tostationid_max=rentals_df['To station id'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Bike ID={}:{} | Trip ID={}:{} | From ID={}:{} | To ID={}:{}'.format(bikeid_min,bikeid_max,\\\n",
    "                                    tripid_min,tripid_max,fromstationid_min, fromstationid_max,\\\n",
    "                                    tostationid_min, tostationid_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the line, the samples, and the nearest vectors to the plane\n",
    "#xx, yy = np.meshgrid(np.linspace(fromstationid_min, fromstationid_max, 50), \\\n",
    "#                     np.linspace(tostationid_min, tostationid_max, 50))\n",
    "rand_smpl_xx = [ rentals_df['From station id'][i] for i in \\\n",
    "                sorted(np.random.choice(rentals_df['From station id'], size=50, replace=False)) ]\n",
    "rand_smpl_yy = [ rentals_df['To station id'][i] for i in \\\n",
    "                sorted(np.random.choice(rentals_df['To station id'], size=50, replace=False)) ]\n",
    "xx, yy = np.meshgrid(rand_smpl_xx, rand_smpl_yy)\n",
    "xxyy_combo = np.c_[xx.ravel(), yy.ravel()]\n",
    "Z = clf.decision_function(xxyy_combo)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.title(\"Isolation Forest\")\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Blues_r)\n",
    "\n",
    "a1 = rentals_df.loc[rentals_df['anomaly_if'] == 0, ['From station id', 'To station id']] # normal\n",
    "a2 = rentals_df.loc[rentals_df['anomaly_if'] == 1, ['From station id', 'To station id']] #anomaly\n",
    "\n",
    "b1 = plt.scatter(a1['From station id'], a1['To station id'], c='green', s=20, edgecolor='k')\n",
    "b2 = plt.scatter(a2['From station id'], a2['To station id'], c='red', s=20, edgecolor='k')\n",
    "plt.axis('tight')\n",
    "plt.xlim((fromstationid_min, fromstationid_max))\n",
    "plt.ylim((tostationid_min, tostationid_max))\n",
    "plt.legend([b1, b2],\n",
    "           [\"regular data\",\n",
    "            \"anomalies\"],\n",
    "           loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "\n",
    "# Generate train data\n",
    "X = 0.3 * rng.randn(100, 2)\n",
    "X_train = np.r_[X + 2, X - 2]\n",
    "# Generate some regular novel observations\n",
    "X = 0.3 * rng.randn(20, 2)\n",
    "X_test = np.r_[X + 2, X - 2]\n",
    "# Generate some abnormal novel observations\n",
    "X_outliers = rng.uniform(low=-4, high=4, size=(20, 2))\n",
    "\n",
    "# fit the model\n",
    "clf = IsolationForest(max_samples=100, random_state=rng)\n",
    "clf.fit(X_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "y_pred_outliers = clf.predict(X_outliers)\n",
    "\n",
    "# plot the line, the samples, and the nearest vectors to the plane\n",
    "xx, yy = np.meshgrid(np.linspace(-5, 5, 50), np.linspace(-5, 5, 50))\n",
    "xxyy_combo=np.c_[xx.ravel(), yy.ravel()]\n",
    "print(xxyy_combo)\n",
    "Z = clf.decision_function(xxyy_combo)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.title(\"IsolationForest\")\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Blues_r)\n",
    "\n",
    "b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white',\n",
    "                 s=20, edgecolor='k')\n",
    "b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='green',\n",
    "                 s=20, edgecolor='k')\n",
    "c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='red',\n",
    "                s=20, edgecolor='k')\n",
    "plt.axis('tight')\n",
    "plt.xlim((-5, 5))\n",
    "plt.ylim((-5, 5))\n",
    "plt.legend([b1, b2, c],\n",
    "           [\"training observations\",\n",
    "            \"new regular observations\", \"new abnormal observations\"],\n",
    "           loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "((52*52)-52)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "station_df = pd.read_csv('stations.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "station_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rentals_df.groupby(['From station id','To station id']).size().reset_index().rename(columns={0:'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rentals_df.drop_duplicates(subset=['From station id','To station id'])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
